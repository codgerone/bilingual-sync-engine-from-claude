# 对话记录：批量处理优化方案讨论

**日期**：2026-01-18
**参与者**：Meiqi + Claude (Opus 4.5)
**主题**：mapper.py 批量处理优化方案设计

---

## 对话背景

在之前的对话中，已经完成了：
- extractor.py 重构（V2 语义驱动方案）
- mapper.py 清理和 Prompt Caching 优化

本次对话聚焦于：mapper.py 的批量处理优化

---

## 对话全文

### Meiqi 的问题

> 对于mapper.py，我现在有另一个疑惑：目前的映射的方法是对每一行的内容，都单独调用一次API进行映射，对吧？但是，每一行都单独启用一次API，不会很麻烦吗？不会浪费token和降低处理速度吗？（我联想的是，每一次计算，都要重启计算机进行处理）既然从extractor.py中已经得到了结构化的所有行的数据，为什么不在一次API调用中就全部完成映射呢？对于每一行的处理方式都是一致的呀

---

### Claude 的初步回应

这是一个非常好的问题！你的直觉是对的——批量处理确实可能更高效。让我先解释当前设计的考量，然后我们可以讨论是否需要优化。

#### 单行调用 vs 批量调用的权衡

| 方案 | 优点 | 缺点 |
|------|------|------|
| **单行调用**（当前） | 输出简单易解析；单行失败不影响其他行；调试容易 | 多次调用有延迟；重复发送 prompt（已用 Caching 缓解） |
| **批量调用** | 只需一次调用，速度快；可能更省 token | 输出复杂，解析易出错；一次失败可能全批失败；长文档可能超上下文限制 |

#### 当前设计的理由

1. **输出可靠性**：让 LLM 一次输出 100 个 JSON 对象，格式出错的概率远高于输出 1 个
2. **错误隔离**：第 5 行映射失败，第 6-100 行仍能继续
3. **上下文限制**：Claude 有 token 上限，超长文档可能放不下

#### 但你的想法值得考虑

可以采用**折中方案**：每次处理 5-10 行，而不是 1 行或全部。

---

### Meiqi 的追问

> 你先告诉我，对于一篇100行（或者其他量级）的文档，单行调用API和批量调用一次API，如果不触发你所说的格式错误可能性和上下文长度限制，在token消耗和处理速度上会有多大差异？

---

### Claude 的数据分析

#### Token 估算

根据 `_build_system_prompt()` 和 `_build_user_message()` 的内容：

| 部分 | 估算 Token 数 |
|------|--------------|
| **System Prompt**（固定）| ~800 tokens |
| **User Message**（每行）| ~150 tokens |
| **Output**（每行）| ~100 tokens |

#### 对比：100 行文档

**方案 A：单行调用 + Prompt Caching（当前）**

```
第 1 次调用：
  - 缓存写入成本：800 × 1.25 = 1000 tokens
  - User message：150 tokens
  - Output：100 tokens

第 2-100 次调用（每次）：
  - 缓存读取成本：800 × 0.1 = 80 tokens
  - User message：150 tokens
  - Output：100 tokens
```

**总计**：
- 输入：1000 + 150 + 99×(80+150) = **~24,000 tokens**
- 输出：100 × 100 = **10,000 tokens**
- API 调用次数：**100 次**
- 处理时间：每次 1-2 秒 → **100-200 秒**

**方案 B：批量调用（一次处理 100 行）**

```
1 次调用：
  - System prompt：800 tokens
  - User message：150 × 100 = 15,000 tokens
  - Output：100 × 100 = 10,000 tokens
```

**总计**：
- 输入：800 + 15,000 = **~16,000 tokens**
- 输出：**10,000 tokens**
- API 调用次数：**1 次**
- 处理时间：**5-10 秒**

#### 直接对比

| 指标 | 单行调用（当前） | 批量调用 | 差异 |
|------|-----------------|---------|------|
| **输入 Token** | ~24,000 | ~16,000 | 批量**省 33%** |
| **输出 Token** | 10,000 | 10,000 | 相同 |
| **API 调用次数** | 100 次 | 1 次 | 批量**快 100 倍** |
| **处理时间** | 100-200 秒 | 5-10 秒 | 批量**快 10-20 倍** |

#### 结论

你的直觉完全正确！批量调用在 **token 成本**和**处理速度**上都明显优于单行调用：
- Token 成本节省约 **1/3**
- 处理速度快约 **10-20 倍**

**那为什么当前还用单行调用？** 主要是历史原因——V1 版本设计时更关注可靠性和调试方便，没有优先考虑效率。

---

### Meiqi 的请求

> 设计一个批量处理的优化方案，说说你的思考，我们一起讨论，最终出一版现阶段最合适的批量处理方案

---

### Claude 的方案设计

#### 核心问题

设计批量处理需要考虑几个关键问题：

| 问题 | 考量 |
|------|------|
| 一次处理多少行？ | 太少没意义，太多有风险 |
| 输入格式怎么设计？ | LLM 需要清晰区分每一行 |
| 输出格式怎么设计？ | 需要能可靠解析每行结果 |
| 出错了怎么办？ | 整批失败如何处理 |
| 超长文档怎么办？ | 超过上下文限制怎么分批 |

#### 问题 1：批次大小

**限制因素**：

1. **输出 Token 限制**：Claude 单次输出上限约 4K-8K tokens
   - 每行输出约 100 tokens
   - 安全上限：约 40-60 行

2. **输出可靠性**：让 LLM 输出的 JSON 数组越长，格式错误概率越高
   - 经验值：10-20 行比较稳定

3. **输入上下文**：Claude 有 200K token 上下文窗口
   - 这个通常不是瓶颈

**Claude 的建议**：默认 **10 行一批**，可配置

```
100 行文档 → 10 次 API 调用（每次 10 行）
vs 当前 100 次 API 调用

处理时间：100-200 秒 → 10-20 秒
```

#### 问题 2：输入格式设计

**方案 A：编号列表**

```
请处理以下 3 行修订：

【第 0 行】
- 修订前：AI正在改变我们的生活
- 修订后：AI改变了我们的生活
- 英文当前：AI is changing our life

【第 1 行】
- 修订前：协议有效期为一年
- 修订后：协议有效期为两年
- 英文当前：The agreement is valid for one year

【第 2 行】
...
```

**方案 B：JSON 输入**

```json
[
  {"row": 0, "before": "...", "after": "...", "current": "..."},
  {"row": 1, "before": "...", "after": "...", "current": "..."}
]
```

**Claude 倾向方案 A**：对 LLM 更友好，可读性强，不容易混淆

#### 问题 3：输出格式设计

```json
[
  {
    "row": 0,
    "target_after": "AI has changed our life",
    "explanation": "进行时→完成时",
    "confidence": 0.95
  },
  {
    "row": 1,
    "target_after": "The agreement is valid for two years",
    "explanation": "one→two, year→years",
    "confidence": 0.98
  }
]
```

**关键**：输出必须包含 `row` 字段，用于对应输入

#### 问题 4：错误处理策略

**降级机制**：

```
批量处理 10 行
      ↓
   成功？──是──→ 返回结果
      │
      否
      ↓
   JSON 解析失败
      ↓
   降级：逐行重试这 10 行
```

这样既享受批量的效率，又保证单行失败不影响其他行。

#### 问题 5：动态批次（进阶优化）

根据文本长度动态调整批次大小：

```python
def calculate_batch_size(row_pairs):
    avg_length = 平均每行字符数
    if avg_length < 100:
        return 20  # 短文本，多批量
    elif avg_length < 300:
        return 10  # 中等文本
    else:
        return 5   # 长文本，少批量
```

**Claude 的建议**：现阶段先用固定批次（10行），后续再加动态调整

#### Claude 的总结推荐方案

| 项目 | 方案 |
|------|------|
| 批次大小 | 固定 10 行（可配置） |
| 输入格式 | 编号列表（方案 A） |
| 输出格式 | JSON 数组，含 row 字段 |
| 错误处理 | 批量失败 → 逐行降级重试 |
| 动态批次 | 暂不实现，后续优化 |

#### Claude 向 Meiqi 提出的问题

1. **批次大小**：你觉得 10 行一批合适吗？还是想更激进（20行）或更保守（5行）？
2. **降级策略**：批量失败时逐行重试，你觉得这个策略合理吗？还是有其他想法？
3. **还有其他担心的点吗**？比如某些特殊情况我没考虑到的？

---

### Meiqi 的深度反馈

> 我来讲讲我的想法：
>
> 1. 首先，我认为处理速度是至关重要的。在现在这个时代，人们越来越没有耐心等待了。我记得google之前做过这样一个统计：将搜索结果页生成的数据增加0.5秒，流量约会下降20%。仅不到一秒的延迟就可能导致用户留存率的大幅下降。基于此，我认为一个产品的处理速度是至关重要的。我认为针对速度，最合适的原则是，能有多快就要做到多快。由速度快可能导致的问题应想其他办法去解决。
>
> 2. 基于速度最重要原则，我认为调用API进行映射时，我们应以token生成的上限为限，进行单次API调用。（不需要考虑处理多少行，因为这东西拿捏不准，有的情况一行可能存在很多字符，需要消耗很多token，有的则不然。所以我认为直接把单次API的输出终止在输出上限是最明智的选择）
>
> 3. 接下来就是要设计发生错误时的响应机制：
>    - (1) 我认为针对错误，我们最首要的态度应该是尽可能规避错误的发生。也就是说，在进行mapping之前，或者说在extractor阶段，我们就应该尽最大可能避免接下来在映射时发生错误。这里我想知道，你认为可能会出现哪些错误？如果我们要设计在mapping之前的预处理机制，应该如何设计？或者如何在extractor阶段就规避掉错误？
>    - (2) 第二是考虑错误真的发生时该如何处理。我认为你的降级策略并不符合速度最重要原则以及资源损耗最小原则。批量处理失败时，如果降级逐行重试，那没有错误的行相当于也需要占用资源重新处理。这完全没有必要。降级重新处理应该只针对出错的行。
>    - (3) 第三要考虑的是在生成token消耗到接近上限时，正在处理的最后一行的错误问题。我认为，如果我们以token消耗的上限为界限，那就很难实现正好完成处理完最后一行的情况。所以正在处理的最后一行应该默认为会发生错误，需要加入到错误处理机制中去。
>
> 4. 对于输入格式设计，我认为直接输入extractor阶段得到的JSON数据为最佳。需要做的写好清晰的prompt，让LLM知道应该如何处理这些JSON数据。这是最节省资源和时间的方法。用你提出的方案A的话，相当于要对extractor阶段的到的JSON数据先进行一次预处理，在进行真正的映射处理，浪费时间，浪费资源。
>
> 以上是我对批量处理的优化方案的一些想法。我需要告诉你，我在产品设计和具体的工程师实现上都没有过系统性的学习和认识，我所提出的问题和想法都只是依靠我的直觉提出的，所以我希望你能站在专业AI应用产品从业者的角度来审视和分析这些观点，我完全有可能是有错误的或者是有视野盲区的。同时你应该也注意到我只提出了观点，没有提出解决方法。所以你也需要思考最优的解决方法并为我讲解。请你仔细思考，并输出你的意见和方案吧。

---

### Claude 的专业分析

#### 首先，一个重要的技术澄清

你提到"以 token 生成的上限为限"——这个想法很有创意，但有一个关键误解：

**我们无法控制 LLM 输出多少 token，只能控制输入。**

```
你能控制的          你不能控制的
    ↓                   ↓
┌─────────┐        ┌─────────┐
│ 输入    │   →    │ 输出    │
│ (发送   │  LLM   │ (LLM    │
│  多少行)│        │  生成)  │
└─────────┘        └─────────┘
```

- `max_tokens` 是上限，不是目标
- LLM 会根据任务"自然地"生成输出
- 如果输出超过上限 → **被截断** → JSON 不完整 → 解析失败

**例如**：
```json
[{"row": 0, "target_after": "..."}, {"row": 1, "target_aft   ← 截断了！
```

所以正确的做法是：**控制输入行数，使预期输出不超过上限**

---

#### 逐点回应你的想法

##### 1. 速度至上原则 ✅ 完全同意

你说得对。先追求快，再解决快带来的问题。这是正确的产品思维。

##### 2. 以输出上限为界 → 修正为"动态估算批次"

既然无法精确控制输出，我们可以**估算**：

```
每行预期输出 ≈ 100-150 tokens（基于文本长度）
安全输出上限 = max_tokens × 80%（留余量）
批次大小 = 安全上限 / 每行预期输出

例：max_tokens=4000，每行预期 150 tokens
    批次大小 = 4000 × 0.8 / 150 ≈ 21 行
```

**动态调整**：
- 如果实际输出 > 90% 上限 → 下次减少批次
- 如果实际输出 < 50% 上限 → 下次增加批次

这样既追求速度，又保持安全。

##### 3. 错误处理

**(1) 预防错误 —— 可能的错误类型**

| 错误类型 | 来源 | 预防方法 |
|---------|------|---------|
| 空文本 | extractor | 过滤空行 |
| 超长文本 | extractor | 检测 >1000 字符的行，单独处理 |
| 特殊字符 | extractor | 清理或转义 |
| JSON 格式错误 | LLM 输出 | 清晰的 prompt + 示例 |
| 行号对应错误 | LLM 输出 | prompt 强调必须包含 row 字段 |
| 截断 | 批次过大 | 动态批次 + 截断检测 |

**extractor 阶段的预处理**：
```python
def preprocess_row_pairs(row_pairs):
    valid = []
    for pair in row_pairs:
        # 过滤空行
        if not pair['source_before'].strip():
            continue
        # 标记超长行（后续单独处理）
        if len(pair['source_before']) > 1000:
            pair['_long_text'] = True
        valid.append(pair)
    return valid
```

**(2) 只重试出错的行 ✅ 完全同意**

你说得对，我之前的降级策略有问题。

**改进方案：容错解析**

```python
def parse_batch_response(response_text):
    """尝试解析，提取所有成功的行"""

    # 尝试完整解析
    try:
        results = json.loads(response_text)
        return results, []  # 全部成功，无失败
    except:
        pass

    # 完整解析失败 → 逐个提取
    successful = []
    failed_rows = []

    # 用正则找所有 {"row": X, ...} 对象
    pattern = r'\{"row":\s*(\d+)[^}]+\}'
    for match in re.finditer(pattern, response_text):
        try:
            obj = json.loads(match.group())
            successful.append(obj)
        except:
            failed_rows.append(int(match.group(1)))

    return successful, failed_rows
```

**重试策略**：
```
批量处理 20 行
      ↓
解析结果：18 行成功，2 行失败（row 5, row 12）
      ↓
只重试 row 5 和 row 12（小批次，更保守）
```

**(3) 截断检测 ✅ 好问题**

API 响应会告诉我们是否被截断：

```python
response = client.messages.create(...)

if response.stop_reason == "end_turn":
    # 正常完成
    pass
elif response.stop_reason == "max_tokens":
    # 被截断！最后一行可能不完整
    # 解析时丢弃最后一个不完整对象
    # 将其加入重试队列
```

##### 4. 输入格式用 JSON ✅ 同意

你说得对，转换格式是浪费资源。直接用 extractor 输出的 JSON。

---

#### 综合方案

```
┌─────────────────────────────────────────────────────────────┐
│                    批量映射流程                              │
└─────────────────────────────────────────────────────────────┘

1. 预处理（extractor 阶段）
   ├── 过滤空行
   ├── 标记超长行
   └── 验证数据完整性

2. 动态分批
   ├── 估算每行输出 token 数
   ├── 计算安全批次大小（目标：输出 < 80% 上限）
   └── 超长行单独处理

3. 批量调用 API
   ├── 直接发送 JSON 格式输入
   └── 使用 Prompt Caching（system prompt 缓存）

4. 容错解析
   ├── 尝试完整 JSON 解析
   ├── 失败则逐个提取成功的行
   └── 记录失败的 row 号

5. 截断检测
   ├── 检查 stop_reason
   └── 如被截断，丢弃最后不完整行，加入重试

6. 精准重试
   ├── 只重试失败的行
   └── 用更小批次（更保守）

7. 自适应调整
   ├── 记录每次调用的输出 token 数
   └── 动态调整下次批次大小
```

---

#### Claude 向 Meiqi 提出的问题

1. **关于动态批次**：你觉得这个"估算 + 动态调整"的方案能接受吗？还是你有其他想法来解决"无法精确控制输出"的问题？

2. **关于超长行**：超过 1000 字符的行，你觉得应该单独处理（一行一调用），还是也尝试批量？

3. **关于重试次数**：一行最多重试几次？我建议 2-3 次，超过就标记为失败，让用户手动处理。你觉得呢？

---

## 对话结束

Meiqi 表示将本次讨论内容与其他 LLM 进行探讨，"偏听则暗，兼听则明"。

---

## 待讨论的问题

1. 动态批次的"估算 + 动态调整"方案是否可行？
2. 超过 1000 字符的超长行如何处理？
3. 单行最多重试几次？

---

## 关键学习点

1. **LLM API 的限制**：我们只能控制输入，无法精确控制输出长度
2. **速度至上原则**：先追求快，再解决快带来的问题
3. **错误预防优先**：在上游（extractor）就规避下游（mapper）的错误
4. **精准重试**：只重试失败的行，不浪费资源重试成功的行
5. **容错解析**：即使 JSON 整体解析失败，也尝试提取成功的部分
